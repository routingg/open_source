{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/routingg/open_source/blob/master/selfie2anime_testset(%EC%9A%B0%EB%A6%AC%EC%82%AC%EC%A7%84)%2C_epoch100%2C_%EC%B5%9C%EC%A2%85%20%EC%8B%A4%ED%8C%A8%20ver_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JyNOzXg8RXX"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWlApOfk7qbo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WGGYJQg9NAW"
      },
      "source": [
        "**Custom Dataset** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56a7MLt08O2t"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, img_dir):\n",
        "        img_dir = BASE_DATASET_PATH + \"/\" + img_dir + \"/\"\n",
        "        \n",
        "        path_list = os.listdir(img_dir)\n",
        "        abspath = os.path.abspath(img_dir) \n",
        "        \n",
        "        self.img_dir = img_dir\n",
        "        self.img_list = [os.path.join(abspath, path) for path in path_list]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(IMG_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]), # normalize image between -1 and 1\n",
        "        ])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.img_list[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "\n",
        "        img_tensor = self.transform(img)\n",
        "        return img_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZwczjOs9jzm"
      },
      "source": [
        "**Discriminator Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nON0Tf7B8O67"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,conv_dim=32):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, conv_dim, 4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(conv_dim, conv_dim*2, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(conv_dim*2, conv_dim*4, 4, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(conv_dim*4, conv_dim*8, 4, padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(conv_dim*8, 1, 4, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)\n",
        "        x = F.avg_pool2d(x, x.size()[2:])\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KG6O0eL9-v9"
      },
      "source": [
        "**ResidualBlock Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YA0aKtW8O-Z"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels, 3),\n",
        "            nn.InstanceNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels, 3),\n",
        "            nn.InstanceNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI4EH1pt-CwL"
      },
      "source": [
        "**Generator Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MjPgjM28PCB"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, conv_dim=64, n_res_block=9):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(3, conv_dim, 7),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(conv_dim, conv_dim*2, 3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim*2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(conv_dim*2, conv_dim*4, 3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim*4),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            ResidualBlock(conv_dim*4),\n",
        "            \n",
        "            nn.ConvTranspose2d(conv_dim*4, conv_dim*2, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim*2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(conv_dim*2, conv_dim, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.InstanceNorm2d(conv_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(conv_dim, 3, 7),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR9MPBDd-8jk"
      },
      "source": [
        "**CycleGAN Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeDKXANF-8vc"
      },
      "outputs": [],
      "source": [
        "class CycleGAN:\n",
        "\n",
        "    def __init__(self, g_conv_dim=64, d_conv_dim=64, n_res_block=6):\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "        self.G_XtoY = Generator(conv_dim=g_conv_dim, n_res_block=n_res_block).to(self.device)\n",
        "        self.G_YtoX = Generator(conv_dim=g_conv_dim, n_res_block=n_res_block).to(self.device)\n",
        "\n",
        "        self.D_X = Discriminator(conv_dim=d_conv_dim).to(self.device)\n",
        "        self.D_Y = Discriminator(conv_dim=d_conv_dim).to(self.device)\n",
        "\n",
        "        print(f\"Models running of {self.device}\")\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
        "        return torch.load(save_filename)\n",
        "\n",
        "    def real_mse_loss(self, D_out):\n",
        "        return torch.mean((D_out-1)**2)\n",
        "\n",
        "\n",
        "    def fake_mse_loss(self, D_out):\n",
        "        return torch.mean(D_out**2)\n",
        "\n",
        "\n",
        "    def cycle_consistency_loss(self, real_img, reconstructed_img, lambda_weight):\n",
        "        reconstr_loss = torch.mean(torch.abs(real_img - reconstructed_img))\n",
        "        return lambda_weight*reconstr_loss\n",
        "    \n",
        "    def train_generator(self, optimizers, images_x, images_y):\n",
        "        # Generator YtoX\n",
        "        optimizers[\"g_optim\"].zero_grad()\n",
        "\n",
        "        fake_images_x = self.G_YtoX(images_y)\n",
        "\n",
        "        d_real_x = self.D_X(fake_images_x)\n",
        "        g_YtoX_loss = self.real_mse_loss(d_real_x)\n",
        "\n",
        "        recon_y = self.G_XtoY(fake_images_x)\n",
        "        recon_y_loss = self.cycle_consistency_loss(images_y, recon_y, lambda_weight=10)\n",
        "\n",
        "\n",
        "        # Generator XtoY\n",
        "        fake_images_y = self.G_XtoY(images_x)\n",
        "\n",
        "        d_real_y = self.D_Y(fake_images_y)\n",
        "        g_XtoY_loss = self.real_mse_loss(d_real_y)\n",
        "\n",
        "        recon_x = self.G_YtoX(fake_images_y)\n",
        "        recon_x_loss = self.cycle_consistency_loss(images_x, recon_x, lambda_weight=10)\n",
        "\n",
        "        g_total_loss = g_YtoX_loss + g_XtoY_loss + recon_y_loss + recon_x_loss\n",
        "        g_total_loss.backward()\n",
        "        optimizers[\"g_optim\"].step()\n",
        "\n",
        "        return g_total_loss.item()\n",
        "\n",
        "    \n",
        "    def train_discriminator(self, optimizers, images_x, images_y):\n",
        "        # Discriminator x\n",
        "        optimizers[\"d_x_optim\"].zero_grad()\n",
        "\n",
        "        d_real_x = self.D_X(images_x)\n",
        "        d_real_loss_x = self.real_mse_loss(d_real_x)\n",
        "        \n",
        "        fake_images_x = self.G_YtoX(images_y)\n",
        "\n",
        "        d_fake_x = self.D_X(fake_images_x)\n",
        "        d_fake_loss_x = self.fake_mse_loss(d_fake_x)\n",
        "        \n",
        "        d_x_loss = d_real_loss_x + d_fake_loss_x\n",
        "        d_x_loss.backward()\n",
        "        optimizers[\"d_x_optim\"].step()\n",
        "\n",
        "\n",
        "        # Discriminator y\n",
        "        optimizers[\"d_y_optim\"].zero_grad()            \n",
        "        d_real_y = self.D_Y(images_y)\n",
        "        d_real_loss_x = self.real_mse_loss(d_real_y)\n",
        "    \n",
        "        fake_images_y = self.G_XtoY(images_x)\n",
        "\n",
        "        d_fake_y = self.D_Y(fake_images_y)\n",
        "        d_fake_loss_y = self.fake_mse_loss(d_fake_y)\n",
        "\n",
        "        d_y_loss = d_real_loss_x + d_fake_loss_y\n",
        "        d_y_loss.backward()\n",
        "        optimizers[\"d_y_optim\"].step()\n",
        "\n",
        "        return d_x_loss.item(), d_y_loss.item()\n",
        "\n",
        "\n",
        "    def train(self, optimizers, data_loader_x, data_loader_y, print_every=10, sample_every=100):\n",
        "        losses = []\n",
        "        g_total_loss_min = np.Inf\n",
        "    \n",
        "        fixed_x = next(iter(data_loader_x))[1].to(self.device)\n",
        "        fixed_y = next(iter(data_loader_y))[1].to(self.device)\n",
        "\n",
        "        print(f'Running on {self.device}')\n",
        "        for epoch in range(EPOCHS):\n",
        "            for (images_x, images_y) in zip(data_loader_x, data_loader_y):\n",
        "                images_x, images_y = images_x.to(self.device), images_y.to(self.device)\n",
        "                \n",
        "                g_total_loss = self.train_generator(optimizers, images_x, images_y)\n",
        "                d_x_loss, d_y_loss = self.train_discriminator(optimizers, images_x, images_y)\n",
        "                \n",
        "            \n",
        "            if epoch % print_every == 0:\n",
        "                losses.append((d_x_loss, d_y_loss, g_total_loss))\n",
        "                print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'\n",
        "                .format(\n",
        "                    epoch, \n",
        "                    EPOCHS, \n",
        "                    d_x_loss, \n",
        "                    d_y_loss, \n",
        "                    g_total_loss\n",
        "                ))\n",
        "                \n",
        "            if g_total_loss < g_total_loss_min:\n",
        "                g_total_loss_min = g_total_loss\n",
        "                \n",
        "                torch.save(self.G_XtoY.state_dict(), \"G_X2Y\")\n",
        "                torch.save(self.G_YtoX.state_dict(), \"G_Y2X\")\n",
        "                                \n",
        "                torch.save(self.D_X.state_dict(), \"D_X\")\n",
        "                torch.save(self.D_Y.state_dict(), \"D_Y\")\n",
        "                \n",
        "                print(\"Models Saved\")\n",
        "                \n",
        "                \n",
        "\n",
        "        return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw7sh2GZ_mLJ"
      },
      "source": [
        "**Config**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YxCcna-cQi6h",
        "outputId": "ec3858fe-bb36-4eb8-b562-931d5a91e5dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm28yN6k_mXT"
      },
      "outputs": [],
      "source": [
        "BASE_DATASET_PATH = '/content/drive/MyDrive/selfie2anime'\n",
        "X_DATASET = \"trainA\"\n",
        "Y_DATASET = \"trainB\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "N_WORKERS = 0\n",
        "\n",
        "IMG_SIZE = 128\n",
        "LR = 0.0002\n",
        "BETA1 = 0.5\n",
        "BETA2 = 0.999\n",
        "\n",
        "EPOCHS = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVPTUDgN_reA"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H1sZtI6-_rqJ",
        "outputId": "97f1daf9-8692-4c55-9007-08a446ba4565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models running of cuda\n",
            "Running on cuda\n",
            "Epoch [    0/  100] | d_X_loss: 0.3900 | d_Y_loss: 0.3944 | g_total_loss: 5.7236\n",
            "Models Saved\n",
            "Epoch [    1/  100] | d_X_loss: 0.2660 | d_Y_loss: 0.2554 | g_total_loss: 5.4657\n",
            "Models Saved\n",
            "Epoch [    2/  100] | d_X_loss: 0.3607 | d_Y_loss: 0.3497 | g_total_loss: 5.2424\n",
            "Models Saved\n",
            "Epoch [    3/  100] | d_X_loss: 0.3733 | d_Y_loss: 0.3228 | g_total_loss: 6.2958\n",
            "Epoch [    4/  100] | d_X_loss: 0.6277 | d_Y_loss: 0.3854 | g_total_loss: 5.9247\n",
            "Epoch [    5/  100] | d_X_loss: 0.7415 | d_Y_loss: 0.4790 | g_total_loss: 4.3227\n",
            "Models Saved\n",
            "Epoch [    6/  100] | d_X_loss: 0.2252 | d_Y_loss: 0.2802 | g_total_loss: 5.8139\n",
            "Epoch [    7/  100] | d_X_loss: 0.6236 | d_Y_loss: 0.5980 | g_total_loss: 4.7137\n",
            "Epoch [    8/  100] | d_X_loss: 0.2165 | d_Y_loss: 0.2161 | g_total_loss: 5.4541\n",
            "Epoch [    9/  100] | d_X_loss: 0.2810 | d_Y_loss: 0.3102 | g_total_loss: 4.9639\n",
            "Epoch [   10/  100] | d_X_loss: 0.3368 | d_Y_loss: 0.9564 | g_total_loss: 6.9945\n",
            "Epoch [   11/  100] | d_X_loss: 0.2438 | d_Y_loss: 0.3500 | g_total_loss: 5.0752\n",
            "Epoch [   12/  100] | d_X_loss: 0.4159 | d_Y_loss: 0.6693 | g_total_loss: 6.2528\n",
            "Epoch [   13/  100] | d_X_loss: 0.3223 | d_Y_loss: 0.3614 | g_total_loss: 4.4891\n",
            "Epoch [   14/  100] | d_X_loss: 0.2961 | d_Y_loss: 0.3015 | g_total_loss: 4.4415\n",
            "Epoch [   15/  100] | d_X_loss: 0.2519 | d_Y_loss: 0.2938 | g_total_loss: 4.4451\n",
            "Epoch [   16/  100] | d_X_loss: 0.3878 | d_Y_loss: 0.3147 | g_total_loss: 4.9684\n",
            "Epoch [   17/  100] | d_X_loss: 0.4527 | d_Y_loss: 0.3118 | g_total_loss: 5.1497\n",
            "Epoch [   18/  100] | d_X_loss: 0.2252 | d_Y_loss: 0.4647 | g_total_loss: 4.4239\n",
            "Epoch [   19/  100] | d_X_loss: 0.2907 | d_Y_loss: 0.3481 | g_total_loss: 5.0829\n",
            "Epoch [   20/  100] | d_X_loss: 0.3916 | d_Y_loss: 0.3827 | g_total_loss: 4.2487\n",
            "Models Saved\n",
            "Epoch [   21/  100] | d_X_loss: 0.3495 | d_Y_loss: 0.3314 | g_total_loss: 4.0733\n",
            "Models Saved\n",
            "Epoch [   22/  100] | d_X_loss: 0.4773 | d_Y_loss: 0.5522 | g_total_loss: 4.3073\n",
            "Epoch [   23/  100] | d_X_loss: 0.3382 | d_Y_loss: 0.3332 | g_total_loss: 4.5683\n",
            "Epoch [   24/  100] | d_X_loss: 0.2878 | d_Y_loss: 0.3981 | g_total_loss: 3.8794\n",
            "Models Saved\n",
            "Epoch [   25/  100] | d_X_loss: 0.3901 | d_Y_loss: 0.2750 | g_total_loss: 4.8348\n",
            "Epoch [   26/  100] | d_X_loss: 0.3061 | d_Y_loss: 0.3154 | g_total_loss: 5.3206\n",
            "Epoch [   27/  100] | d_X_loss: 0.3036 | d_Y_loss: 0.4230 | g_total_loss: 4.1457\n",
            "Epoch [   28/  100] | d_X_loss: 0.3246 | d_Y_loss: 0.3355 | g_total_loss: 4.5556\n",
            "Epoch [   29/  100] | d_X_loss: 0.3093 | d_Y_loss: 0.3549 | g_total_loss: 3.8485\n",
            "Models Saved\n",
            "Epoch [   30/  100] | d_X_loss: 0.4011 | d_Y_loss: 0.3508 | g_total_loss: 3.6345\n",
            "Models Saved\n",
            "Epoch [   31/  100] | d_X_loss: 0.3176 | d_Y_loss: 0.3271 | g_total_loss: 4.4941\n",
            "Epoch [   32/  100] | d_X_loss: 0.2776 | d_Y_loss: 0.5328 | g_total_loss: 5.5567\n",
            "Epoch [   33/  100] | d_X_loss: 0.2853 | d_Y_loss: 0.3058 | g_total_loss: 4.2692\n",
            "Epoch [   34/  100] | d_X_loss: 0.3833 | d_Y_loss: 0.3879 | g_total_loss: 5.1354\n",
            "Epoch [   35/  100] | d_X_loss: 0.3878 | d_Y_loss: 0.3137 | g_total_loss: 4.2443\n",
            "Epoch [   36/  100] | d_X_loss: 0.2238 | d_Y_loss: 0.2796 | g_total_loss: 4.1300\n",
            "Epoch [   37/  100] | d_X_loss: 0.2918 | d_Y_loss: 0.2945 | g_total_loss: 3.9491\n",
            "Epoch [   38/  100] | d_X_loss: 0.2072 | d_Y_loss: 0.3297 | g_total_loss: 4.3055\n",
            "Epoch [   39/  100] | d_X_loss: 0.2234 | d_Y_loss: 0.5217 | g_total_loss: 3.7379\n",
            "Epoch [   40/  100] | d_X_loss: 0.3097 | d_Y_loss: 0.2944 | g_total_loss: 4.3506\n",
            "Epoch [   41/  100] | d_X_loss: 0.5220 | d_Y_loss: 0.2021 | g_total_loss: 4.1582\n",
            "Epoch [   42/  100] | d_X_loss: 0.5031 | d_Y_loss: 0.4978 | g_total_loss: 3.1528\n",
            "Models Saved\n",
            "Epoch [   43/  100] | d_X_loss: 0.4917 | d_Y_loss: 0.5035 | g_total_loss: 3.3163\n",
            "Epoch [   44/  100] | d_X_loss: 0.5499 | d_Y_loss: 0.2012 | g_total_loss: 4.3993\n",
            "Epoch [   45/  100] | d_X_loss: 0.6717 | d_Y_loss: 0.3282 | g_total_loss: 3.6780\n",
            "Epoch [   46/  100] | d_X_loss: 0.3057 | d_Y_loss: 0.1472 | g_total_loss: 4.5353\n",
            "Epoch [   47/  100] | d_X_loss: 0.3175 | d_Y_loss: 0.2987 | g_total_loss: 4.1818\n",
            "Epoch [   48/  100] | d_X_loss: 0.2997 | d_Y_loss: 0.5052 | g_total_loss: 5.2499\n",
            "Epoch [   49/  100] | d_X_loss: 0.1889 | d_Y_loss: 0.2083 | g_total_loss: 4.6931\n",
            "Epoch [   50/  100] | d_X_loss: 0.2905 | d_Y_loss: 0.2956 | g_total_loss: 4.8085\n",
            "Epoch [   51/  100] | d_X_loss: 0.3837 | d_Y_loss: 0.7901 | g_total_loss: 5.8172\n",
            "Epoch [   52/  100] | d_X_loss: 0.2855 | d_Y_loss: 0.1990 | g_total_loss: 4.5983\n",
            "Epoch [   53/  100] | d_X_loss: 0.3114 | d_Y_loss: 0.5216 | g_total_loss: 4.4993\n",
            "Epoch [   54/  100] | d_X_loss: 0.2079 | d_Y_loss: 0.2339 | g_total_loss: 4.7291\n",
            "Epoch [   55/  100] | d_X_loss: 0.2413 | d_Y_loss: 0.3211 | g_total_loss: 3.9966\n",
            "Epoch [   56/  100] | d_X_loss: 0.2721 | d_Y_loss: 0.2326 | g_total_loss: 4.7861\n",
            "Epoch [   57/  100] | d_X_loss: 0.3237 | d_Y_loss: 0.1666 | g_total_loss: 4.2359\n",
            "Epoch [   58/  100] | d_X_loss: 0.2746 | d_Y_loss: 0.2965 | g_total_loss: 3.8505\n",
            "Epoch [   59/  100] | d_X_loss: 0.2687 | d_Y_loss: 0.2944 | g_total_loss: 4.2545\n",
            "Epoch [   60/  100] | d_X_loss: 0.3441 | d_Y_loss: 0.2672 | g_total_loss: 3.3454\n",
            "Epoch [   61/  100] | d_X_loss: 0.3065 | d_Y_loss: 0.3717 | g_total_loss: 4.5652\n",
            "Epoch [   62/  100] | d_X_loss: 0.3063 | d_Y_loss: 0.3521 | g_total_loss: 3.6401\n",
            "Epoch [   63/  100] | d_X_loss: 0.2184 | d_Y_loss: 0.3732 | g_total_loss: 4.1010\n",
            "Epoch [   64/  100] | d_X_loss: 0.2469 | d_Y_loss: 0.2176 | g_total_loss: 4.4551\n",
            "Epoch [   65/  100] | d_X_loss: 0.1947 | d_Y_loss: 0.2056 | g_total_loss: 4.0943\n",
            "Epoch [   66/  100] | d_X_loss: 0.2168 | d_Y_loss: 0.2263 | g_total_loss: 5.0996\n",
            "Epoch [   67/  100] | d_X_loss: 0.2402 | d_Y_loss: 0.2420 | g_total_loss: 4.4327\n",
            "Epoch [   68/  100] | d_X_loss: 0.2492 | d_Y_loss: 0.1907 | g_total_loss: 4.2489\n",
            "Epoch [   69/  100] | d_X_loss: 0.1914 | d_Y_loss: 0.2360 | g_total_loss: 5.0139\n",
            "Epoch [   70/  100] | d_X_loss: 0.3151 | d_Y_loss: 0.1658 | g_total_loss: 4.7150\n",
            "Epoch [   71/  100] | d_X_loss: 0.1967 | d_Y_loss: 5.0189 | g_total_loss: 10.5317\n",
            "Epoch [   72/  100] | d_X_loss: 0.3227 | d_Y_loss: 0.5874 | g_total_loss: 3.5113\n",
            "Epoch [   73/  100] | d_X_loss: 0.2749 | d_Y_loss: 0.5544 | g_total_loss: 3.0969\n",
            "Models Saved\n",
            "Epoch [   74/  100] | d_X_loss: 0.1633 | d_Y_loss: 0.5410 | g_total_loss: 3.4675\n",
            "Epoch [   75/  100] | d_X_loss: 0.2078 | d_Y_loss: 0.5128 | g_total_loss: 2.8935\n",
            "Models Saved\n",
            "Epoch [   76/  100] | d_X_loss: 0.2794 | d_Y_loss: 0.4887 | g_total_loss: 3.4852\n",
            "Epoch [   77/  100] | d_X_loss: 0.1564 | d_Y_loss: 0.4993 | g_total_loss: 3.2469\n",
            "Epoch [   78/  100] | d_X_loss: 0.3592 | d_Y_loss: 0.5578 | g_total_loss: 3.8632\n",
            "Epoch [   79/  100] | d_X_loss: 0.2266 | d_Y_loss: 0.5943 | g_total_loss: 2.7602\n",
            "Models Saved\n",
            "Epoch [   80/  100] | d_X_loss: 0.2432 | d_Y_loss: 0.5272 | g_total_loss: 3.7015\n",
            "Epoch [   81/  100] | d_X_loss: 0.4264 | d_Y_loss: 0.5036 | g_total_loss: 4.1651\n",
            "Epoch [   82/  100] | d_X_loss: 0.1861 | d_Y_loss: 0.5508 | g_total_loss: 3.5423\n",
            "Epoch [   83/  100] | d_X_loss: 0.3351 | d_Y_loss: 0.6300 | g_total_loss: 2.5571\n",
            "Models Saved\n",
            "Epoch [   84/  100] | d_X_loss: 0.1787 | d_Y_loss: 0.5266 | g_total_loss: 3.1530\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-61c798cbf713>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycleGan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-f523c6f02c2d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizers, data_loader_x, data_loader_y, print_every, sample_every)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running on {self.device}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mimages_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f1605764937f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/selfie2anime/trainA/female_28492.jpg'"
          ]
        }
      ],
      "source": [
        "# Dataset\n",
        "x_dataset = Dataset(X_DATASET)\n",
        "y_dataset = Dataset(Y_DATASET)\n",
        "\n",
        "data_loader_x = DataLoader(x_dataset, BATCH_SIZE, shuffle=True, num_workers=N_WORKERS)\n",
        "data_loader_y = DataLoader(y_dataset, BATCH_SIZE, shuffle=True, num_workers=N_WORKERS)\n",
        "\n",
        "# Model\n",
        "cycleGan = CycleGAN()\n",
        "\n",
        "# Oprimizer\n",
        "g_params = list(cycleGan.G_XtoY.parameters()) + list(cycleGan.G_YtoX.parameters())\n",
        "\n",
        "optimizers = {\n",
        "    \"g_optim\": optim.Adam(g_params, LR, [BETA1, BETA2]),\n",
        "    \"d_x_optim\": optim.Adam(cycleGan.D_X.parameters(), LR, [BETA1, BETA2]),\n",
        "    \"d_y_optim\": optim.Adam(cycleGan.D_Y.parameters(), LR, [BETA1, BETA2])\n",
        "}\n",
        "\n",
        "# Train\n",
        "losses = cycleGan.train(optimizers, data_loader_x, data_loader_y, print_every=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9PDtvfDhTQ_"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses.T[0], label='Discriminator, X', alpha=0.5)\n",
        "plt.plot(losses.T[1], label='Discriminator, Y', alpha=0.5)\n",
        "plt.plot(losses.T[2], label='Generators', alpha=0.5)\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# from PIL import Image\n",
        "# import os\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "# # Google Drive 마운트\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # 이미지 파일이 있는 폴더 경로\n",
        "# image_folder = '/content/drive/MyDrive/selfie2anime/testA'\n",
        "\n",
        "# # 전처리된 이미지를 저장할 폴더 경로\n",
        "# save_dir = '/content/drive/MyDrive/selfie2anime/preprocessed'\n",
        "\n",
        "# # 이미지 전처리를 위한 Transform\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((128, 128)),\n",
        "# ])\n",
        "\n",
        "# # 이미지 파일 리스트 가져오기\n",
        "# image_files = os.listdir(image_folder)\n",
        "\n",
        "# # 폴더 생성\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# # 이미지 파일 전처리 및 저장\n",
        "# for i, image_file in enumerate(image_files):\n",
        "#     image_path = os.path.join(image_folder, image_file)\n",
        "#     image = Image.open(image_path)\n",
        "#     image = transform(image)\n",
        "#     image = transforms.functional.pad(image, (0, 0, 0, 128 - image.size[1], 0, 128 - image.size[0]))\n",
        "\n",
        "#     save_path = os.path.join(save_dir, f'image_{i}.jpg')\n",
        "#     image.save(save_path)\n",
        "\n",
        "# print('이미지 전처리 및 저장 완료')\n"
      ],
      "metadata": {
        "id": "IKvUXtNm1WXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28F2k44uhV0z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "4ee62ef9-83a5-43b9-ddd2-d40c815f756b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c54cdc495abb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_loader_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Test Dataset\n",
        "x_dataset = Dataset(\"testset\")\n",
        "y_dataset = Dataset(\"testB\")\n",
        "\n",
        "data_loader_x = DataLoader(x_dataset, BATCH_SIZE, shuffle=True, num_workers=N_WORKERS)\n",
        "data_loader_y = DataLoader(y_dataset, BATCH_SIZE, shuffle=True, num_workers=N_WORKERS)\n",
        "\n",
        "samples = []\n",
        "\n",
        "for i in range(12):\n",
        "    fixed_x = next(iter(data_loader_x))[i].to(cycleGan.device)\n",
        "    fake_y = cycleGan.G_XtoY(torch.unsqueeze(fixed_x, dim=0))\n",
        "    samples.extend([fixed_x, torch.squeeze(fake_y, 0)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xusSils_hZXz"
      },
      "source": [
        "**Test Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ2VglAShV7_"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(18, 14))\n",
        "grid = ImageGrid(fig, 111, nrows_ncols=(5, 4), axes_pad=0.5)\n",
        "\n",
        "\n",
        "for i, (ax, im) in enumerate(zip(grid, samples)):\n",
        "    _, w, h = im.size()\n",
        "    im = im.detach().cpu().numpy()\n",
        "    im = np.transpose(im, (1, 2, 0))\n",
        "    \n",
        "    im = ((im +1)*255 / (2)).astype(np.uint8)\n",
        "    ax.imshow(im.reshape((w,h,3)))\n",
        "\n",
        "    ax.xaxis.set_visible(False)\n",
        "    ax.yaxis.set_visible(False)\n",
        "\n",
        "    if i%2 == 0: title = \"selfie\"\n",
        "    else: title = \"anime\"\n",
        "\n",
        "    ax.set_title(title)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XckTQLRuhV-b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}